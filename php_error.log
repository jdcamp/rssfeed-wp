[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: microsoft not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:07:36 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: microsoft not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:08:11 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: microsoft not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:37 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: microsoft not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:09:47 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: microsoft not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:10:39 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: microsoft not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:11:37 UTC] keyword: turing not found
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : Expanding ARAM Predictions
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : What I learned from Photo Bot
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*VkTa1FIbLMaLcBO5MMAjbQ.jpeg"/></figure><p>Last night at the AI panel a few wise folks brought up the topic of BI.</p><p>Is AI = BI? Is BI = AI? Is one a subset of the other or are they completely different? Let’s unpack some of this.</p><p>AI manifests itself in the form of software. Software is algorithms coded using a programming language. From that perspective, AI = BI. They are all just Turing Machines.</p><p>Digging deeper, however, <strong>there is a difference between the set of algorithms being typically applied to AI products vs BI products.</strong> One can use a model like Linear Regression in both AI and BI but the variety and depth of models and algorithms being applied to AI products is much richer than BI. Most BI products are a combination of SQL queries and joins. I’d say BI is not as sophisticated as AI (although it’s not her fault if the creators don’t fully exploit it).</p><p><strong>The data quantity standards are lower for BI.</strong> This is a complement, not a criticism. Since BI doesn’t advertise complex statistical models (Machine Learning, Deep Learning), it’s humble and is willing to do simple analytics on whatever data is available. AI, on the other hand, simply isn’t applicable if there’s not enough data. You can really can’t call it AI if your dataset has 37 rows of data. Please don’t do that.</p><p><strong>BI is almost always tied to dashboards.</strong> There’s almost always a visual element associated with BI products. With AI, while some applications like Predictive Marketing &amp; Sales have a rich visual layer in the form of dashboards, a lot of AI quietly works behind the scenes and you may never even notice it.</p><p>New interfaces that don’t rely on a black screen (think Alexa and family) are being associated with AI and not BI. But that’s because of the next point.</p><p><strong>Perception — this is likely the biggest difference.</strong> BI is jaded. BI is tired. It’s been around many years and has its share of successes and failures. It simply doesn’t have the wow factor anymore. We, as a species, are driven by what’s new and shiny. BI isn’t new and shiny. There are no BI startups making news. None of the big tech companies are talking up BI. They are all talking up AI. There’s “social proof” that AI is the thing to do. You wouldn’t even be reading this if the title was “Learn more about BI today”.</p><p>In sales &amp; marketing, <strong>AI and BI have the potential of working together.</strong> Data from an AI-driven product is consumed by different types of BI users at the organization. At one company we’re working with, some users will be consuming <a href="http://www.salesting.com/" rel="noreferrer" target="_blank">SalesTing</a> data into Microsoft PowerBI. At another one, it’s Tableau. AI and BI have already become friends.</p><p><strong>Organizational structures</strong> — most companies already have a BI team and I don’t see them renaming it to an AI team anytime soon unless they are a company selling AI products. BI teams are typically an exclusive club. They do the heavy analytics and inform management who then make decisions that impact all employees. With AI, many products will simply have AI built-in and each employee will use without ever realizing (or caring) what it is. AI will directly touch more humans than BI.</p><p><strong>Conclusion — I’m going to take a stand. AI is a superset of BI.</strong></p><p>Deep in my heart, however, they are all Turing machines. That’s one truth that will stand the test of time.</p><p><a href="https://medium.com/salesting/ai-vs-bi-1642aa371f82" rel="noreferrer" target="_blank">AI vs BI</a> was originally published in <a href="https://medium.com/salesting" rel="noreferrer" target="_blank">SalesTing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : AI vs BI
[22-Jun-2017 20:12:42 UTC] keyword: microsoft not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : Deep learning for facial analysis
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:12:42 UTC] keyword: turing not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : Expanding ARAM Predictions
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : What I learned from Photo Bot
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*VkTa1FIbLMaLcBO5MMAjbQ.jpeg"/></figure><p>Last night at the AI panel a few wise folks brought up the topic of BI.</p><p>Is AI = BI? Is BI = AI? Is one a subset of the other or are they completely different? Let’s unpack some of this.</p><p>AI manifests itself in the form of software. Software is algorithms coded using a programming language. From that perspective, AI = BI. They are all just Turing Machines.</p><p>Digging deeper, however, <strong>there is a difference between the set of algorithms being typically applied to AI products vs BI products.</strong> One can use a model like Linear Regression in both AI and BI but the variety and depth of models and algorithms being applied to AI products is much richer than BI. Most BI products are a combination of SQL queries and joins. I’d say BI is not as sophisticated as AI (although it’s not her fault if the creators don’t fully exploit it).</p><p><strong>The data quantity standards are lower for BI.</strong> This is a complement, not a criticism. Since BI doesn’t advertise complex statistical models (Machine Learning, Deep Learning), it’s humble and is willing to do simple analytics on whatever data is available. AI, on the other hand, simply isn’t applicable if there’s not enough data. You can really can’t call it AI if your dataset has 37 rows of data. Please don’t do that.</p><p><strong>BI is almost always tied to dashboards.</strong> There’s almost always a visual element associated with BI products. With AI, while some applications like Predictive Marketing &amp; Sales have a rich visual layer in the form of dashboards, a lot of AI quietly works behind the scenes and you may never even notice it.</p><p>New interfaces that don’t rely on a black screen (think Alexa and family) are being associated with AI and not BI. But that’s because of the next point.</p><p><strong>Perception — this is likely the biggest difference.</strong> BI is jaded. BI is tired. It’s been around many years and has its share of successes and failures. It simply doesn’t have the wow factor anymore. We, as a species, are driven by what’s new and shiny. BI isn’t new and shiny. There are no BI startups making news. None of the big tech companies are talking up BI. They are all talking up AI. There’s “social proof” that AI is the thing to do. You wouldn’t even be reading this if the title was “Learn more about BI today”.</p><p>In sales &amp; marketing, <strong>AI and BI have the potential of working together.</strong> Data from an AI-driven product is consumed by different types of BI users at the organization. At one company we’re working with, some users will be consuming <a href="http://www.salesting.com/" rel="noreferrer" target="_blank">SalesTing</a> data into Microsoft PowerBI. At another one, it’s Tableau. AI and BI have already become friends.</p><p><strong>Organizational structures</strong> — most companies already have a BI team and I don’t see them renaming it to an AI team anytime soon unless they are a company selling AI products. BI teams are typically an exclusive club. They do the heavy analytics and inform management who then make decisions that impact all employees. With AI, many products will simply have AI built-in and each employee will use without ever realizing (or caring) what it is. AI will directly touch more humans than BI.</p><p><strong>Conclusion — I’m going to take a stand. AI is a superset of BI.</strong></p><p>Deep in my heart, however, they are all Turing machines. That’s one truth that will stand the test of time.</p><p><a href="https://medium.com/salesting/ai-vs-bi-1642aa371f82" rel="noreferrer" target="_blank">AI vs BI</a> was originally published in <a href="https://medium.com/salesting" rel="noreferrer" target="_blank">SalesTing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : AI vs BI
[22-Jun-2017 20:13:39 UTC] keyword: microsoft not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : Deep learning for facial analysis
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:13:39 UTC] keyword: turing not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : Expanding ARAM Predictions
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : What I learned from Photo Bot
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*VkTa1FIbLMaLcBO5MMAjbQ.jpeg"/></figure><p>Last night at the AI panel a few wise folks brought up the topic of BI.</p><p>Is AI = BI? Is BI = AI? Is one a subset of the other or are they completely different? Let’s unpack some of this.</p><p>AI manifests itself in the form of software. Software is algorithms coded using a programming language. From that perspective, AI = BI. They are all just Turing Machines.</p><p>Digging deeper, however, <strong>there is a difference between the set of algorithms being typically applied to AI products vs BI products.</strong> One can use a model like Linear Regression in both AI and BI but the variety and depth of models and algorithms being applied to AI products is much richer than BI. Most BI products are a combination of SQL queries and joins. I’d say BI is not as sophisticated as AI (although it’s not her fault if the creators don’t fully exploit it).</p><p><strong>The data quantity standards are lower for BI.</strong> This is a complement, not a criticism. Since BI doesn’t advertise complex statistical models (Machine Learning, Deep Learning), it’s humble and is willing to do simple analytics on whatever data is available. AI, on the other hand, simply isn’t applicable if there’s not enough data. You can really can’t call it AI if your dataset has 37 rows of data. Please don’t do that.</p><p><strong>BI is almost always tied to dashboards.</strong> There’s almost always a visual element associated with BI products. With AI, while some applications like Predictive Marketing &amp; Sales have a rich visual layer in the form of dashboards, a lot of AI quietly works behind the scenes and you may never even notice it.</p><p>New interfaces that don’t rely on a black screen (think Alexa and family) are being associated with AI and not BI. But that’s because of the next point.</p><p><strong>Perception — this is likely the biggest difference.</strong> BI is jaded. BI is tired. It’s been around many years and has its share of successes and failures. It simply doesn’t have the wow factor anymore. We, as a species, are driven by what’s new and shiny. BI isn’t new and shiny. There are no BI startups making news. None of the big tech companies are talking up BI. They are all talking up AI. There’s “social proof” that AI is the thing to do. You wouldn’t even be reading this if the title was “Learn more about BI today”.</p><p>In sales &amp; marketing, <strong>AI and BI have the potential of working together.</strong> Data from an AI-driven product is consumed by different types of BI users at the organization. At one company we’re working with, some users will be consuming <a href="http://www.salesting.com/" rel="noreferrer" target="_blank">SalesTing</a> data into Microsoft PowerBI. At another one, it’s Tableau. AI and BI have already become friends.</p><p><strong>Organizational structures</strong> — most companies already have a BI team and I don’t see them renaming it to an AI team anytime soon unless they are a company selling AI products. BI teams are typically an exclusive club. They do the heavy analytics and inform management who then make decisions that impact all employees. With AI, many products will simply have AI built-in and each employee will use without ever realizing (or caring) what it is. AI will directly touch more humans than BI.</p><p><strong>Conclusion — I’m going to take a stand. AI is a superset of BI.</strong></p><p>Deep in my heart, however, they are all Turing machines. That’s one truth that will stand the test of time.</p><p><a href="https://medium.com/salesting/ai-vs-bi-1642aa371f82" rel="noreferrer" target="_blank">AI vs BI</a> was originally published in <a href="https://medium.com/salesting" rel="noreferrer" target="_blank">SalesTing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : AI vs BI
[22-Jun-2017 20:14:43 UTC] keyword: microsoft not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : Deep learning for facial analysis
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:14:43 UTC] keyword: turing not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : Expanding ARAM Predictions
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : What I learned from Photo Bot
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*VkTa1FIbLMaLcBO5MMAjbQ.jpeg"/></figure><p>Last night at the AI panel a few wise folks brought up the topic of BI.</p><p>Is AI = BI? Is BI = AI? Is one a subset of the other or are they completely different? Let’s unpack some of this.</p><p>AI manifests itself in the form of software. Software is algorithms coded using a programming language. From that perspective, AI = BI. They are all just Turing Machines.</p><p>Digging deeper, however, <strong>there is a difference between the set of algorithms being typically applied to AI products vs BI products.</strong> One can use a model like Linear Regression in both AI and BI but the variety and depth of models and algorithms being applied to AI products is much richer than BI. Most BI products are a combination of SQL queries and joins. I’d say BI is not as sophisticated as AI (although it’s not her fault if the creators don’t fully exploit it).</p><p><strong>The data quantity standards are lower for BI.</strong> This is a complement, not a criticism. Since BI doesn’t advertise complex statistical models (Machine Learning, Deep Learning), it’s humble and is willing to do simple analytics on whatever data is available. AI, on the other hand, simply isn’t applicable if there’s not enough data. You can really can’t call it AI if your dataset has 37 rows of data. Please don’t do that.</p><p><strong>BI is almost always tied to dashboards.</strong> There’s almost always a visual element associated with BI products. With AI, while some applications like Predictive Marketing &amp; Sales have a rich visual layer in the form of dashboards, a lot of AI quietly works behind the scenes and you may never even notice it.</p><p>New interfaces that don’t rely on a black screen (think Alexa and family) are being associated with AI and not BI. But that’s because of the next point.</p><p><strong>Perception — this is likely the biggest difference.</strong> BI is jaded. BI is tired. It’s been around many years and has its share of successes and failures. It simply doesn’t have the wow factor anymore. We, as a species, are driven by what’s new and shiny. BI isn’t new and shiny. There are no BI startups making news. None of the big tech companies are talking up BI. They are all talking up AI. There’s “social proof” that AI is the thing to do. You wouldn’t even be reading this if the title was “Learn more about BI today”.</p><p>In sales &amp; marketing, <strong>AI and BI have the potential of working together.</strong> Data from an AI-driven product is consumed by different types of BI users at the organization. At one company we’re working with, some users will be consuming <a href="http://www.salesting.com/" rel="noreferrer" target="_blank">SalesTing</a> data into Microsoft PowerBI. At another one, it’s Tableau. AI and BI have already become friends.</p><p><strong>Organizational structures</strong> — most companies already have a BI team and I don’t see them renaming it to an AI team anytime soon unless they are a company selling AI products. BI teams are typically an exclusive club. They do the heavy analytics and inform management who then make decisions that impact all employees. With AI, many products will simply have AI built-in and each employee will use without ever realizing (or caring) what it is. AI will directly touch more humans than BI.</p><p><strong>Conclusion — I’m going to take a stand. AI is a superset of BI.</strong></p><p>Deep in my heart, however, they are all Turing machines. That’s one truth that will stand the test of time.</p><p><a href="https://medium.com/salesting/ai-vs-bi-1642aa371f82" rel="noreferrer" target="_blank">AI vs BI</a> was originally published in <a href="https://medium.com/salesting" rel="noreferrer" target="_blank">SalesTing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : AI vs BI
[22-Jun-2017 20:15:30 UTC] keyword: microsoft not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : Deep learning for facial analysis
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:15:30 UTC] keyword: turing not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : Expanding ARAM Predictions
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : What I learned from Photo Bot
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*VkTa1FIbLMaLcBO5MMAjbQ.jpeg"/></figure><p>Last night at the AI panel a few wise folks brought up the topic of BI.</p><p>Is AI = BI? Is BI = AI? Is one a subset of the other or are they completely different? Let’s unpack some of this.</p><p>AI manifests itself in the form of software. Software is algorithms coded using a programming language. From that perspective, AI = BI. They are all just Turing Machines.</p><p>Digging deeper, however, <strong>there is a difference between the set of algorithms being typically applied to AI products vs BI products.</strong> One can use a model like Linear Regression in both AI and BI but the variety and depth of models and algorithms being applied to AI products is much richer than BI. Most BI products are a combination of SQL queries and joins. I’d say BI is not as sophisticated as AI (although it’s not her fault if the creators don’t fully exploit it).</p><p><strong>The data quantity standards are lower for BI.</strong> This is a complement, not a criticism. Since BI doesn’t advertise complex statistical models (Machine Learning, Deep Learning), it’s humble and is willing to do simple analytics on whatever data is available. AI, on the other hand, simply isn’t applicable if there’s not enough data. You can really can’t call it AI if your dataset has 37 rows of data. Please don’t do that.</p><p><strong>BI is almost always tied to dashboards.</strong> There’s almost always a visual element associated with BI products. With AI, while some applications like Predictive Marketing &amp; Sales have a rich visual layer in the form of dashboards, a lot of AI quietly works behind the scenes and you may never even notice it.</p><p>New interfaces that don’t rely on a black screen (think Alexa and family) are being associated with AI and not BI. But that’s because of the next point.</p><p><strong>Perception — this is likely the biggest difference.</strong> BI is jaded. BI is tired. It’s been around many years and has its share of successes and failures. It simply doesn’t have the wow factor anymore. We, as a species, are driven by what’s new and shiny. BI isn’t new and shiny. There are no BI startups making news. None of the big tech companies are talking up BI. They are all talking up AI. There’s “social proof” that AI is the thing to do. You wouldn’t even be reading this if the title was “Learn more about BI today”.</p><p>In sales &amp; marketing, <strong>AI and BI have the potential of working together.</strong> Data from an AI-driven product is consumed by different types of BI users at the organization. At one company we’re working with, some users will be consuming <a href="http://www.salesting.com/" rel="noreferrer" target="_blank">SalesTing</a> data into Microsoft PowerBI. At another one, it’s Tableau. AI and BI have already become friends.</p><p><strong>Organizational structures</strong> — most companies already have a BI team and I don’t see them renaming it to an AI team anytime soon unless they are a company selling AI products. BI teams are typically an exclusive club. They do the heavy analytics and inform management who then make decisions that impact all employees. With AI, many products will simply have AI built-in and each employee will use without ever realizing (or caring) what it is. AI will directly touch more humans than BI.</p><p><strong>Conclusion — I’m going to take a stand. AI is a superset of BI.</strong></p><p>Deep in my heart, however, they are all Turing machines. That’s one truth that will stand the test of time.</p><p><a href="https://medium.com/salesting/ai-vs-bi-1642aa371f82" rel="noreferrer" target="_blank">AI vs BI</a> was originally published in <a href="https://medium.com/salesting" rel="noreferrer" target="_blank">SalesTing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : AI vs BI
[22-Jun-2017 20:16:06 UTC] keyword: microsoft not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : Deep learning for facial analysis
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:16:06 UTC] keyword: turing not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : Expanding ARAM Predictions
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : What I learned from Photo Bot
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*VkTa1FIbLMaLcBO5MMAjbQ.jpeg"/></figure><p>Last night at the AI panel a few wise folks brought up the topic of BI.</p><p>Is AI = BI? Is BI = AI? Is one a subset of the other or are they completely different? Let’s unpack some of this.</p><p>AI manifests itself in the form of software. Software is algorithms coded using a programming language. From that perspective, AI = BI. They are all just Turing Machines.</p><p>Digging deeper, however, <strong>there is a difference between the set of algorithms being typically applied to AI products vs BI products.</strong> One can use a model like Linear Regression in both AI and BI but the variety and depth of models and algorithms being applied to AI products is much richer than BI. Most BI products are a combination of SQL queries and joins. I’d say BI is not as sophisticated as AI (although it’s not her fault if the creators don’t fully exploit it).</p><p><strong>The data quantity standards are lower for BI.</strong> This is a complement, not a criticism. Since BI doesn’t advertise complex statistical models (Machine Learning, Deep Learning), it’s humble and is willing to do simple analytics on whatever data is available. AI, on the other hand, simply isn’t applicable if there’s not enough data. You can really can’t call it AI if your dataset has 37 rows of data. Please don’t do that.</p><p><strong>BI is almost always tied to dashboards.</strong> There’s almost always a visual element associated with BI products. With AI, while some applications like Predictive Marketing &amp; Sales have a rich visual layer in the form of dashboards, a lot of AI quietly works behind the scenes and you may never even notice it.</p><p>New interfaces that don’t rely on a black screen (think Alexa and family) are being associated with AI and not BI. But that’s because of the next point.</p><p><strong>Perception — this is likely the biggest difference.</strong> BI is jaded. BI is tired. It’s been around many years and has its share of successes and failures. It simply doesn’t have the wow factor anymore. We, as a species, are driven by what’s new and shiny. BI isn’t new and shiny. There are no BI startups making news. None of the big tech companies are talking up BI. They are all talking up AI. There’s “social proof” that AI is the thing to do. You wouldn’t even be reading this if the title was “Learn more about BI today”.</p><p>In sales &amp; marketing, <strong>AI and BI have the potential of working together.</strong> Data from an AI-driven product is consumed by different types of BI users at the organization. At one company we’re working with, some users will be consuming <a href="http://www.salesting.com/" rel="noreferrer" target="_blank">SalesTing</a> data into Microsoft PowerBI. At another one, it’s Tableau. AI and BI have already become friends.</p><p><strong>Organizational structures</strong> — most companies already have a BI team and I don’t see them renaming it to an AI team anytime soon unless they are a company selling AI products. BI teams are typically an exclusive club. They do the heavy analytics and inform management who then make decisions that impact all employees. With AI, many products will simply have AI built-in and each employee will use without ever realizing (or caring) what it is. AI will directly touch more humans than BI.</p><p><strong>Conclusion — I’m going to take a stand. AI is a superset of BI.</strong></p><p>Deep in my heart, however, they are all Turing machines. That’s one truth that will stand the test of time.</p><p><a href="https://medium.com/salesting/ai-vs-bi-1642aa371f82" rel="noreferrer" target="_blank">AI vs BI</a> was originally published in <a href="https://medium.com/salesting" rel="noreferrer" target="_blank">SalesTing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : AI vs BI
[22-Jun-2017 20:16:43 UTC] keyword: microsoft not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : Deep learning for facial analysis
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:16:43 UTC] keyword: turing not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : Expanding ARAM Predictions
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : What I learned from Photo Bot
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:17:41 UTC] __keyword: MICROSOFTfound in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*VkTa1FIbLMaLcBO5MMAjbQ.jpeg"/></figure><p>Last night at the AI panel a few wise folks brought up the topic of BI.</p><p>Is AI = BI? Is BI = AI? Is one a subset of the other or are they completely different? Let’s unpack some of this.</p><p>AI manifests itself in the form of software. Software is algorithms coded using a programming language. From that perspective, AI = BI. They are all just Turing Machines.</p><p>Digging deeper, however, <strong>there is a difference between the set of algorithms being typically applied to AI products vs BI products.</strong> One can use a model like Linear Regression in both AI and BI but the variety and depth of models and algorithms being applied to AI products is much richer than BI. Most BI products are a combination of SQL queries and joins. I’d say BI is not as sophisticated as AI (although it’s not her fault if the creators don’t fully exploit it).</p><p><strong>The data quantity standards are lower for BI.</strong> This is a complement, not a criticism. Since BI doesn’t advertise complex statistical models (Machine Learning, Deep Learning), it’s humble and is willing to do simple analytics on whatever data is available. AI, on the other hand, simply isn’t applicable if there’s not enough data. You can really can’t call it AI if your dataset has 37 rows of data. Please don’t do that.</p><p><strong>BI is almost always tied to dashboards.</strong> There’s almost always a visual element associated with BI products. With AI, while some applications like Predictive Marketing &amp; Sales have a rich visual layer in the form of dashboards, a lot of AI quietly works behind the scenes and you may never even notice it.</p><p>New interfaces that don’t rely on a black screen (think Alexa and family) are being associated with AI and not BI. But that’s because of the next point.</p><p><strong>Perception — this is likely the biggest difference.</strong> BI is jaded. BI is tired. It’s been around many years and has its share of successes and failures. It simply doesn’t have the wow factor anymore. We, as a species, are driven by what’s new and shiny. BI isn’t new and shiny. There are no BI startups making news. None of the big tech companies are talking up BI. They are all talking up AI. There’s “social proof” that AI is the thing to do. You wouldn’t even be reading this if the title was “Learn more about BI today”.</p><p>In sales &amp; marketing, <strong>AI and BI have the potential of working together.</strong> Data from an AI-driven product is consumed by different types of BI users at the organization. At one company we’re working with, some users will be consuming <a href="http://www.salesting.com/" rel="noreferrer" target="_blank">SalesTing</a> data into Microsoft PowerBI. At another one, it’s Tableau. AI and BI have already become friends.</p><p><strong>Organizational structures</strong> — most companies already have a BI team and I don’t see them renaming it to an AI team anytime soon unless they are a company selling AI products. BI teams are typically an exclusive club. They do the heavy analytics and inform management who then make decisions that impact all employees. With AI, many products will simply have AI built-in and each employee will use without ever realizing (or caring) what it is. AI will directly touch more humans than BI.</p><p><strong>Conclusion — I’m going to take a stand. AI is a superset of BI.</strong></p><p>Deep in my heart, however, they are all Turing machines. That’s one truth that will stand the test of time.</p><p><a href="https://medium.com/salesting/ai-vs-bi-1642aa371f82" rel="noreferrer" target="_blank">AI vs BI</a> was originally published in <a href="https://medium.com/salesting" rel="noreferrer" target="_blank">SalesTing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:17:41 UTC] keyword: MICROSOFT not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : Deep learning for facial analysis
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:17:41 UTC] keyword: TURING not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : Expanding ARAM Predictions
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : What I learned from Photo Bot
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:18:24 UTC] keyword: microsoft not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : Deep learning for facial analysis
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:18:24 UTC] keyword: turing not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PtmHUHyNveDLJyssQ7yVA.jpeg"/><figcaption>source: www.graymeta.com</figcaption></figure><p>Machine learning is definitely <a href="https://medium.com/towards-data-science/understand-these-5-basic-concepts-to-sound-like-a-machine-learning-expert-6221ec0fe960" rel="noreferrer" target="_blank">VERY cool</a>, much like virtual reality or a touch bar on your keyboard. But there is a big difference between <em>cool</em> and <em>useful</em>. For me, something is useful if it solves a problem, saves me time, or saves me money. Usually, those three things are connected, and relate to a grander idea; <em>Return on Investment</em>.</p><p>There has been some astonishing <a href="https://medium.com/working-for-change/i-didnt-worry-about-ai-taking-over-the-world-until-i-learned-about-this-3a8e15f04269" rel="noreferrer" target="_blank">leaps forward</a> in artificial intelligence and machine learning, but none of it is going to matter if it doesn’t offer a return on your investment. So how do you make machine learning useful? Here are some real life examples of how machine learning is saving companies time and money:</p><ol><li>Find stuff</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*L4ziWxCqCpV3V-fK_JQQ2A.jpeg"/><figcaption><a href="https://www.thunderstone.com/products-for-search/search-appliance/" rel="noreferrer" target="_blank">https://www.thunderstone.com/products-for-search/search-appliance/</a></figcaption></figure><p>I’m sure you’ve spent time looking for a picture or an e-mail. If you added it all up, how much time was it? How much money do you get paid per hour? Companies have this problem as well. We are all totally swamped in digital content. We have files and folders everywhere, and they’re filled to the brim with stuff. To make things worse, we’re not tracking it very well either. Platforms similar to what my company <a href="http://www.graymeta.com/" rel="noreferrer" target="_blank">GrayMeta</a> makes are being used to scan everything businesses have, and run things like object recognition, text analysis, speech to text, face recognition etc. to create nice, searchable databases. There’s a serious reduction in time people now spend on searching for and finding stuff. That savings is much greater than the cost of the platform. Tadaaa! Thats ROI baby.</p><p>2. Target your audience</p><p>One of the biggest problems advertisers have today is people ignoring their product. I admit I find 99% of ads annoying and irrelevant. I go out of my way to not click on or look at ads. The problem is that ads are still too broad, and usually don’t reflect my personal interests. Platforms that advertise want to fix that with machine learning.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/615/1*sE0UZUBq67F4_eIvCnJYJg.png"/><figcaption><a href="http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/" rel="noreferrer" target="_blank">http://neilpatel.com/blog/your-ads-are-getting-ignored-5-smart-strategies-to-overcome-banner-blindness/</a></figcaption></figure><p>Companies that provide content to viewers are now using computer vision and speech to text to understand their own content at a far more granular level than before. This information is then dynamically used to drive what ads you see during or alongside the content. Are you watching a movie about dogs? Don’t be surprised to see an ad about dog food. More relevant ads mean more engagements, more engagements mean more money.</p><p>3. Be more efficient with storage</p><p>Did you know that most cloud storage services have different pricing based on how quickly you want your content? Stuff stored in a place that is instantly accessible costs you about $0.023 per GB. But stuff you don’t mind waiting for costs you about $0.004 per GB. Thats 5x cheaper. News organizations have a lot of interviews, b-roll, and other important footage that they’re moving to the cloud. Lets say they have 100TBs of content. To access that quickly (because news happens fast) they keep 100% of that content on the more expensive tier. That costs them $2300 per month, or $27,600 per year.</p><p>Now, they’re using using machine learning to decide what content should be stored on the more expensive tier. Trending keywords on social media initiate a query in a database that has granular metadata for every video (thanks to machine learning). Positive matches to that query initiate a transfer of that video to the more expensive storage. The company can now store the 100TBs on the cheaper storage, saving them $22,800 per year.</p><p>4. Be even more efficient with storage</p><p>It also costs money to use that 100TBs the company above is storing in the cloud. Let’s assume, that by the end of the year, 100% of that content will have needed to have been downloaded, edited, and used for news production. That will cost $84,000. If you don’t know what is in your cloud storage, you have to download it to find out, and that costs you money. Do you have a folder labeled b-roll with lots of video files that you can’t identify just by the file name alone? Thanks to machine learning, people can know what is in every single video without having to download it. They can pull down the exact file they want, instead of entire folders or projects, saving tens of thousands of dollars per year in egress charges.</p><p>5. Analyze stuff</p><p>Most of machine learning is about predicting things. A popular VOD company takes the list of all the things you’ve watched, when you watched them, what was trending right before you watched, and trains a machine learning model to try and predict what you’re going to watch next. They use this prediction to make sure that that content is already available on the closest server to your location. For you, that means the movie plays quickly and at the highest quality. For the VOD company, that means they don’t have to store everything they own on every server in the world. They only move video content to servers when they think you’ll watch it. The amount of money this saves is extraordinary.</p><p>6. Avoid fines and save face</p><p>The FCC and other governmental bodies can fine broadcasters for <em>indecent</em> or <em>obscene</em> things like nudity, sexual content, or graphic language. Other distribution partners may just have strict rules about what they can or cannot play. You’d think that it would be easy to spot questionable content before you send it to distribution, but it turns out that studios spend upwards of 120 person-hours just to check stuff before it goes out the door! If you pay these people $20 an hour, thats $2400 per movie, per distribution channel! If you consider that every single country is at least 1 channel, then you have things like inflight entertainment, day time television, prime time television, on demand… PER COUNTRY.. it gets insane. Fortunately, machine learning is saving these companies tremendous amounts of time and money by flagging content automatically. Humans are still needed to review and approve, but the amount of time they spend doing this is reduced from weeks to minutes. This is one of the most significant returns on investment in machine learning that I’ve personally seen.</p><p>Machine learning can be a very useful tool in the delivery of your goals as an individual or a massive company. <a href="https://medium.com/towards-data-science/how-to-fail-at-machine-learning-36cf26474398" rel="noreferrer" target="_blank">Figuring out how to glue together some cool tech and real problems isn’t easy</a>. Thats why it is always important to consider the usefulness of ideas and the return on your investments.</p>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : 6 ways people are making money with machine learning
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : <p>I recently managed to achieve a 67% success rate in <a href="https://medium.com/@tectonicdisrupt/aram-prediction-67-accuracy-55baca87aabd" rel="noreferrer" target="_blank">predicting the outcomes of ARAM matches</a>. This was done on a dataset from North American players of League of Legends. Since then, I upgraded my data storage and crawling mechanisms which has allowed me to collect data from other regions. Since Riot’s API limits are on a per-region basis, I was able to collect this data in parallel. I now have data on tens of thousands of matches from each new region: Korea, EU West, and EU North.</p><p>My original 67% model was trained on matches from patch 7.11. Since then, patch 7.12 was released and almost all of the new ARAM matches I collected where on that patch. Out of curiosity, I wanted to see how well my previously learned model would perform on the new data. Surprisingly, it worked better than expected, achieving the same 67% accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*FQhqn8YrmaSTLwQ-esVhxg.png"/><figcaption>Test results from a model trained on Korean players on patch 7.12</figcaption></figure><p>One hypothesis in my last post was that Korean players would produce better training data since they’re known as being more consistent. To my surprise, this Korean-trained, Korean-tested model performed about the same as its North American counterpart. I also attained about a 67% accuracy on my EU North and EU West datasets. I’m still collecting more data for training (because it’s so easy to do now), but I don’t expect it to move the models accuracy much.</p><p>I am still working on gaining a deeper understanding of champions and getting humans to review the mistakes the model made to learn new insights that could lead to better accuracy.</p>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : Expanding ARAM Predictions
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : <h4>Understanding Predictive Modeling and the Meaning of “Black Box” Models</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*OY5fqayRxejI-f53tO-P_g.png"/><figcaption>How Brad Pitt feels about Neural Nets</figcaption></figure><p>Since embarking on my study of data science, the most frequent question I’ve gotten has been: “What is Data Science?” A reasonable question, to be sure, and one I am more than happy to answer concerning a field about which I am so passionate. The cleanest distillation of that answer goes something like this:</p><blockquote>Data Science is the process of using machine learning to build predictive models.</blockquote><p>As a natural follow-up, our astute listener might ask for definition of terms:</p><ol><li>What is machine learning?</li><li>What do you mean by model?</li></ol><p>This post focuses on the answer to the latter. And the answer in this context — again in its simplest form — is that a model is a function; it takes an input and generates an output. To begin, we’ll look at one of the most transparent predictive models, linear regression.</p><h4>Linear Regression</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/1*Ak0MJpnFQilvuWm37RxE1A.jpeg"/><figcaption>The Linear Regression of Game Boy models</figcaption></figure><p>Let’s say we are trying to predict what a student, Taylor, will score on a test. We know what Taylor scored on past tests, <em>and </em>we know how many hours she studied. When she doesn’t study at all, she gets a 79/100 and for every hour she studies, she does three points better. Thus, our equation for this relationship is:</p><pre>Taylor&#039;s_Test_Score = 3 * Hours_Studied + 79</pre><p>This example is about as transparent a model as possible. Why? Because we not only know <em>that</em> hours studied influences Taylor’s scores, we know <em>how</em> it influences Taylor’s scores. Namely, being able to see the coefficient, 3, and the constant, 79, tells us the exact relationship between studying and scores. It’s a simple function: Put in two hours, get a score of 85. Put in 6 hours, get a score of 97.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/628/1*p_oY1_IuJ0Ne6AXZdgSPNw.png"/><figcaption>Our very own predictive model represented graphically</figcaption></figure><p>The real world is, of course, more complicated, so we might begin to add more inputs to our model. Maybe we also use hours slept, hours spent watching tv, and absences from school. If we update our equation, each of these new features will have its own coefficient that describes the relationship of that input to the output. All of these inputs are known in data science parlance as the <em>features </em>of our model, the data we use to make our predictions. This linear regression is now <em>multi-dimensional</em> meaning there are multiple inputs, and we can longer easily visualize it in a 2d plane. The coefficients are telling, though. If our desired outcome is the highest possible test score, then we want to maximize the inputs for our large positive coefficients (likely time spent sleeping and studying) and minimize the inputs on our negative coefficients (tv and absences).</p><p>We can even use binary and categorical data in a linear regression. In my dataset, a beer either is or isn’t made by a macro brewery (e.g. Anheuser Busch, Coors, etc). If it is, the value for the feature is_macrobrew is set to one and in a linear regression, our predicted rating for the beer goes down.</p><p>If linear regression is so transparent can can handle different data types, isn’t it better than a black box model? The answer, unfortunately, is no. As we introduce more features, those features may covary. Thinking back to Taylor, more time studying might mean less time sleeping, and to build a more effective model, we need to account for that by introducing a new feature that accounts for the <em>interaction of those two features. </em>We also need to consider that not all relationships are linear. An additional hour of sleep might uniformly improve the score up to 9 or 10 hours but beyond that could have minimal or adverse influence on the score.</p><h4>Tree-Based and Ensemble Modeling</h4><p>In an attempt to overcome the weaknesses of linear modeling, we might try a tree-based approach. A decision tree can be visualized easily and followed like a flow chart. Take the following example using <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">the iris dataset</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/753/1*8q4Yujp2dB9GcPQrTJnFrw.png"/><figcaption><a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noreferrer" target="_blank">Source</a>: If this were a Game Boy, it’d probably be the clear purple one. You can peek inside, but the relationships aren’t quite as transparent.</figcaption></figure><p>The iris dataset is a classification rather than a regression problem. The model looks at 50 samples of each of three types of iris and attempts to classify which type each sample is. The first line in every box shows which feature the model is splitting on. For the topmost box, if the sepal length is less than ~0.75 cm, the model classifies the sample as class Setosa, and if not, the model passes the sample onto the next split. With any given sample, we can easily follow the logic the model is taking from one split to the next. We’ve also accounted for the covariance and non-linearity issues that plague linear models.</p><p>A random forest is an even more powerful version of this type of model. In a random forest, as the name might suggest, we have multiple decision trees. Each tree sees a random subset of the data chosen with replacement (a process known as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noreferrer" target="_blank">bootstrapping</a>) as well as seeing a random sample of the features. In our forest, we might have 10, 100, 1000, or more trees. Transparency has started to take a serious dip. I do not know the relationship of a feature to the target nor can I follow the path of an individual sample to its outcome. The best I can do now is crack open the model and see the percentage of splits each feature performs.</p><h4>A Brief Word on Neural Nets</h4><p>As we continue up the scale of opacity, we finally arrive at neural networks. I’m going to simplify the various types here for the sake of brevity. The basic functioning of a neural network goes like this:</p><ol><li>Every feature is individually fed into a layer of “neurons”</li><li>Each of these neurons is an independent function (like the one we built to predict Taylor’s test) and feeds the output to the next layer</li><li>The outputs of the final layer are synthesized to yield the prediction</li></ol><p>The user determines the number of layers and the number of neurons in each layer when building the model. As you can imagine, one feature might undergo tens of thousands of manipulations. We have no way of knowing the relationship between our model’s inputs and its outputs. Which leads to the final question…</p><h3><strong>Do we need to see inside the box?</strong></h3><p>Anyone engaging in statistics, perhaps especially in the social sciences, has had the distinction between correlation and causation drilled into them. To return to our earliest example, we know that hours studied and test scores are <em>strongly</em> correlated, but we can not be certain of causation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/459/1*Uny-ym1ashJxGKzn1Gmwhw.png"/><figcaption>As usual, there’s an XKCD for that</figcaption></figure><p>Ultimately for the data scientist, the question we need to ask ourselves is, <em>do we care about causation? </em>The answer, more often than not, is probably no. If the model works, if what we put into the machine feeds us out accurate predictions, that might satisfy our success conditions. Sometimes, however, we might need to understand the precise relationships. Building the model might be more about sussing out those relationships than it is about predicting what’s going to happen even if the accuracy of our model suffers.</p><p>These are the sorts of questions that data scientists ask themselves when building a model and how they go about determining which method to use. In my own project, I am most concerned with the accuracy of my predictions and less interested in understanding why a beer would have a high or a low rating, particularly since it’s largely a matter of taste and public opinion.</p><p>I might not be able to tell you what’s happening in the black box, but I can tell you that it works. And that’s often enough.</p>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : What’s in the Box?! — Chipy Mentorship, pt. 3
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : <blockquote>“<strong>Critical thinking skills…really [set] apart the hackers from the true scientists</strong>, for me…. You must must MUST be able to question every step of your process and every number that you come up with.”</blockquote><p><em>―</em><a href="https://www.linkedin.com/in/jakeporway/" rel="noreferrer" target="_blank"><em>Jake Porway</em></a><em>, Founder and Executive Director of DataKind</em></p><p>If only conducting a churn prediction was like competing in a Kaggle competition. You already have a data set, a great infrastructure, a criterion to measure success of your prediction and your target and features are well-defined. You only have to do some feature engineering and test some algorithms and voila you got something. I wish it was that easy.</p><p>In reality, it’s quite complex. I have been working on Churn in the mobile gaming industry for quite some time and this article will expose some of the complexity related to this kind of prediction. Let’s start slowly with some questions we have to answer before conducting a churn prediction.</p><ol><li><em>What is the goal of the prediction?</em> Is it to understand or to predict users that are likely to churn? This often leads to two different sets of algorithm you will use.</li><li><em>Do you want to predict all your user or only a segment?</em> For example, if you want to predict new user or veteran user, you might not use the same features.</li><li><em>What is the target?</em> It’s interesting, but you could treat churn prediction as a classic binary problem (1 : churn user, 0 : not churn), but you could also consider the Y as the number of sessions played.</li><li><em>What are your features? </em>Are you going to use time dependent feature?Base on your industry knowledge, What are the most important features to predict churn? Are they easily computed base on your current database? Have you considered the famous features RFM?</li><li><em>How do you measure the success of your prediction?</em> If you are considering churn as a binary classification problem, a user that do not churn can be pretty rare. You might be tempted to use the Partial area under the ROC curve (PAUC) rather than the AUC or a custom weighted metric (score = sensitivity * 0.3 + specificity * 0.7).</li><li><em>How do you know when to stop improving your prediction and ship your first version? </em>Once you have establish your score criterion, you must establish a threshold. the threshold value will inform you when too stop your experiment.</li><li><em>How do you push your work in production?</em> Do you need to only push your prediction in a database or do you need to create an application as REST API that makes the prediction in real-time. How do you collaborate with the software engineer.</li><li><em>How do you plan to use your result in order to generate ROI? </em>That is extremely important, yet too often neglected. If a certain user as a high probability of churn, how do you act and can you act? There is no point in shipping something complex, that nobody knows what to do with it.</li><li><em>How are you going to monitor the quality of your prediction in a live setting? </em>Are you going to create a dashboard? Will you create a table in your database containing your log?</li><li><em>What is the maintenance process of your model? </em>Do you intend, to make a change to your model once every month, every quarter? What are you planning to change and what is the history of change.</li><li><em>Which tool are you going to use?</em> In must cases, you will be using either R, Python, Spark. Spark is recommended when you really have big data (10 millions rows to predict). Note that the Spark ML library is limited in contrast to Python Library.</li><li><em>Which Packages are you going to use to make a prediction? </em>There are so many packages are there (sklearn, H2O, TPOT, TensorFlow, Theano…. etc)</li><li><em>What are your deliverable? </em>How to you plan to improve your model over time?</li><li>How do you deal with reengage user ? What if you decided to predict user that will churn within the next 30 days ? What if they come back after 30 days? How do you consider this bias within your analysis.</li><li><em>How do you get the data? </em>Do you need to make batch SQL calls to gather the desired data shape.</li><li><em>How do you intend do clean your data</em>? How do you deal with missing, aberrant and extreme value that can screw up your model.</li><li><em>Which algorithm are you going to use?</em></li></ol><p>Here is the best part, you have not even started to code.</p><p>Links to great articles</p><ul><li>Churn as a regression Problem : <a href="https://www.google.ca/search?q=delta+dna+churn&amp;oq=delta+dna+churn&amp;aqs=chrome..69i57j0l5.3701j0j1&amp;sourceid=chrome&amp;ie=UTF-8" rel="noreferrer" target="_blank">delta dna article</a></li><li>Churn for new user : <a href="http://www.gamasutra.com/view/feature/170472/predicting_churn_datamining_your_.php" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn for veteran user : <a href="http://www.gamasutra.com/view/feature/176747/predicting_churn_when_do_veterans_.php?print=1" rel="noreferrer" target="_blank">gamasutra article</a></li><li>Churn prediction survival analysis : <a href="http://www.siliconstudio.co.jp/rd/4front/pdf/DSAA2016_Churn_Prediction_Montreal.pdf" rel="noreferrer" target="_blank">Silicon Studio</a></li><li>Advance example of churn prediction that even consider reengagement : <a href="http://Seems like an approach that address all problem related to churn  https//ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" rel="noreferrer" target="_blank">click</a></li></ul>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : Insights on Churn Prediction Complexity
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*7Fh0-W71ZbIdJiasNuvCJw.jpeg"/></figure><p>It’s been an interesting week since we <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">first released Photo Bot</a>. I thought I’d share a few insights I’ve gathered from examining how each of the AIs performed in response to the dozens of photos that I personally sent, as well as the ones from my friends and others who signed up to use Photo Bot. As a reminder, here’s the link to the demo: <a href="https://muxgram.com/photobot" rel="noreferrer" target="_blank">https://muxgram.com/photobot</a></p><h3><strong>Google</strong></h3><p>Google is very good at identifying specific details. When I took a photo of a bowl of mussels, Google was the only one that identified the image as “mussels” while Microsoft just saw “food”. Amazon more accurately identified it as seafood — but guessed incorrectly that it was clams. And Google detected mussels as one of its top descriptors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*uFfYZjqiZkrXvtmZ."/></figure><p>Furthermore, Google seems especially good at identifying the make and model of a car. In my <a href="https://medium.com/muxgram/labs-experiment-1-photo-bot-fc4cc617697b" rel="noreferrer" target="_blank">previous post</a>, I cited the fact that Google accurately identified my car as a BMW X3. More recently, as I was walking home after lunch one day, I took a picture of a nice sports car parked on the side of the road, which it correctly identified as a Ferrari 458.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fLBXRLwBoLJTDEyQ."/></figure><p>However, Google doesn’t seem to do as well with older models of cars, like with this classic Mercedes 250SL — it only could identify it as an “antique car” or “luxury vehicle”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*fJ4ngA9jzFpzN1T2."/></figure><h3><strong>Amazon</strong></h3><p>Amazon seemed more erratic in its performance. Sometimes it would be the only one to identify the object correctly, like this one of a peacock.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*lt2GN8DuZKqVl7mB."/></figure><p>But then it would not be able to identify something as simple as a dog in the photo, mistaking it for a potted plant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*QRnXkYOvr9BmfZJX."/></figure><p>Otherwise, Amazon would be in the right ballpark for most general objects, like for this lamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-BNivWtQ4eMqMGCO."/></figure><p>Overall, it does OK for simple objects, but when it tries to be specific it tends to be either very wrong, or very right.</p><h3><strong>Microsoft</strong></h3><p>Microsoft is very good at composing the relationship of general objects. It completely nailed this photo my friend Karen sent in as “a statue of person riding a horse in the city”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*mvw0S91SpstRDks3."/></figure><p>Same thing with this photo from my friend Geoff, while he was driving : “a car driving down a busy highway” was the result.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*CIxm1OouTLIQuftI."/></figure><p>But Microsoft can also create very nonsensical descriptions, like this one of a tow truck: “a yellow and black truck sitting on top of a car”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*bFZcQU2mW2xurjhF."/></figure><p>And because Microsoft tends to look for general objects it’s already familiar with, like a car or a road, it can completely miss the most prominent object in the photo. In this case, it missed the mailbox, and only sees the car parked on the side of a road. (To be fair, in this photo, all three missed the mailbox.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/0*-gqBWOhFLYNP2DMg."/></figure><h3><strong>So where does this all lead?</strong></h3><p>These comparisons make me think about user expectations. When submitting photos, people seemed to want Photo Bot to work like web search — if I send a picture of a restaurant, the AI should identify the specific restaurant. A picture of a skyscraper should identify which skyscraper it is. If there’s a picture of a shampoo bottle, people expect it to identify the specific product.</p><p>That expectation may be a window into connecting the offline and online worlds. If a computer vision AI system could identify a specific product, landmark, location of the image, and do it in real time, now <em>that </em>would be amazing. I think this experiment also points to the fact that AR might be the ideal device to manifest this computer vision AI. Given what I’ve seen so far with Photo Bot, I don’t think it’s ready yet, though it’s probably much closer than the general public might assume.</p><p>If Magic Leap actually delivers on its promise, and Google Lens performs exceptionally well with a very high detection rate in the real world, that could be a killer combination. But I do wonder if initial successes will instead come from enterprise applications, and not consumer products. True, that’s different from what’s happened in the last couple of decades: Google, Facebook, and Amazon all grew into massive consumer product successes. But for decades before that, Microsoft, IBM, Intel, were built on enterprise success. Apple is the anomaly, which has spanned both eras with <em>phenomenal </em>success.</p><p>In the next couple of decades, the emerging technologies like AI, self-driving cars, AR/VR and drones may go back to enterprise success. This is not to say that such technologies won’t reach the average consumer eventually, but it might be enterprise that will create the first successful business model that will define the next generation of tech companies.</p><p>All of this reminds me of the classic 1969 Honeywell ad for the “Kitchen Computer”, when the company was trying (very early!) to sell the notion of a computer for everyday use and it’s predictably awful that the popular use they envisioned was for women in kitchens. I do think companies will try to sell AI, VR, drones, etc. for the mass market from the get-go (in fact they are already doing so), but as with the Kitchen Computer, my guess is that there will be a mismatch on timing between when the average consumer is ready and when the technology is good enough. Those with the resources (Google, Amazon, Apple, Facebook) can stick it out for long-term wins—and startups that dig into this arena too early for consumer use may not make it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/443/1*ssp7-3ARDiQuSlFLSCq4OQ.png"/></figure><p><a href="https://medium.com/muxgram/what-i-learned-from-photo-bot-d337b6f69a6d" rel="noreferrer" target="_blank">What I learned from Photo Bot</a> was originally published in <a href="https://medium.com/muxgram" rel="noreferrer" target="_blank">Muxgram</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : What I learned from Photo Bot
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : <p>Applications that escapade data for decision making are the anodynes of the myriad of business problems today in protean sectors such as healthcare, insurance, finance and social media. However, in legion scenarios, the nature of the available data is either unforthcoming or is not readily palatable. The available data, (specifically text) is grimy — full of heterodox entities and eclectic in nature. In more pragmatic data scenarios, it is redundant — there are instances where text records appear disparate but are tantamount. In this article, I will delineate about DataElixir : An automated framework to clean, standardize and deduplicate the unstructured data set.</p><p>Consider a data set of organization names generated on a user survey which consists of variegated ambiguities such as:</p><p><em>N-grams:</em>IBM corporation private , IBM comrporation limited, IBM limited <br/><em>Spellings: IBM </em>corporation, Ibm1 corparation, ibm pyt ltd<strong><em><br/></em></strong><em>Keywords: IBM Co, IBM Company, ibm#, ibm@ com</em></p><p>DataElixir is a framework to redress this vagueness. It is a complete package to perform large-scale data cleansing and deduplication hastily. The overall architecture of DataElixir is described in the following image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/692/1*QErJWDCzChyQd2FcojOcZA.png"/><figcaption>DataElixir — Architecture</figcaption></figure><p>User configurations bridle the overall workflow. Input text records are first passed through <strong><em>Cleaner Block</em></strong> in order to make them pristine. This block is comprised of three types of functions: entity removal (example: punctuation, stop words, slang etc.), entity replacement (example: abbreviation fixes, custom lookup) and entity standardization (using patterns and expressions).</p><p>The cleansed records are then indexed by <strong><em>Indexer Block</em></strong>. The paramount role of Indexer is to ensure the brisk processing (cleansing and deduplication) of text records. Since every record has to be compared with every other record, the total comparison becomes n*n. Indexer Block uses Lucene to for indexing purposes and returns the closely matched candidate pairs can be referred by quick lookup. Experimentation suggested that Indexer block is able to reduce the overall processing time to one-third.</p><p>The candidates are then compared using <strong><em>Matcher Block</em></strong> which consists of flexible text comparison techniques such as Levenshtein matching, Jaro-Winkler, n-gram matching and phonetics matching. Matcher computes the provides the confidence score as well, which is the indication of how closely two records match.</p><p>For one of my project in academic research, the task was to perform <strong><em>machine translation </em></strong>of all the village names of Asia.I used pre-trained supervised learning algorithms for this purpose. The results were decent, but a there was a lot of redundancy in the translated names. Thus, I created DataElixir as the antidote to the problem. A data set of about one million rows of village names was cleansed and deduplicated in about 3 minutes.</p><p><strong><em>Currently, the source-code is not open-sourced, however, I have released a light version of DataElixir — </em></strong><a href="https://github.com/shivam5992/dupandas" rel="noreferrer" target="_blank"><strong><em>dupandas</em></strong></a><strong><em>, which follows the same architecture.</em></strong></p><p>It is written in pure python. Though it works in a homogeneous manner as DataElixir, there are a few leftover works and obviously scope of improvements. Feel free to check it out and suggest any feedback.</p>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : DataElixir: A framework to work with unstructured data (cleansing and deduplication)
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AzvdYOygf99mJFbwJaTMpw.jpeg"/></figure><p><strong>What is Machine Learning?</strong></p><p>Machine learning is a type of artificial intelligence that is beneficial to both consumers and businesses. While you may have seen artificial intelligence in science fiction movies like 2001: A Space Odyssey, now it’s become a reality. Machine learning allows you to gather data from consumers and use it to predict future behavior.</p><p>Machine learning is already driving some of the best experiences on the Internet. Every time Netflix recommends a show, it’s based on previous viewing habits and the ratings of both the individual who owns the account and those with similar viewing habits. That’s machine learning. And whenever you check Facebook and see a product, account, advertisement, or friend recommendation, that’s Facebook attempting to provide added value to their consumers. Because machine learning isn’t perfect, it doesn’t always hit the mark, but it provides valuable insights and on-target recommendations with surprising regularity.</p><p>Businesses benefit from effective machine learning because it makes it easier for them to offer their consumers exactly what they want.</p><p><strong>The Balance Between Machine Learning and Humanity</strong></p><p>Even with the obvious benefits for consumers and businesses, it is still necessary to carefully balance the predictive power of machine learning with the contextual view that a human being brings. Developers and businesses should approach it from a sense of service to their consumers rather than just manipulative profit engineering.</p><p>Consumers are remarkably adept at identifying when companies steer them toward an outcome with no regard for their best interests. If machine learning is used to urge consumers to make a purchase that is contrary to their base desires, it will ultimately be resisted and disdained. But, if machine learning is used to match a customer’s preferences only with those companies and services with which they’re in exact alignment, then consumers will regard the interaction as valuable, welcomed, and preferred.</p><p><strong>The Five-Step Test &amp; Learn Approach</strong></p><p>Implementing machine learning into your business model doesn’t have to be overwhelming or overly complicated. By keeping the project simple and focused, it is possible to make advances more quickly and grow the project organically. Numerous third-party platforms make it easy to integrate machine learning into advertising. It is also feasible to have a unique application coded for your individual needs.</p><p>Regardless of the method you chose, the five steps for implementation are similar.</p><p><strong>Step One:</strong> Decide what the business objective for the initial test. What are you trying to learn? For example, an objective could be to decrease the number of consumers who abandon their shopping cart.</p><p><strong>Step Two:</strong> Choose which customer segment will be the focus of the test. Will it be based on the total cost of the shopping cart, the gender or age of the user, the total length of time spent shopping, or perhaps consumers who have left items in the cart for a set of time? It is possible to test any or these. But, the more variables you include at one time, the more difficult it will be to determine a causal relationship between action, offer, and subsequent behavior.</p><p><strong>Step Three:</strong> Decide on the hypothesis of the test. What do you think the result of the trial will be? What would an optimal result look like as opposed to a complete failure? Here’s an example hypothesis: If you offer a coupon to consumers after they have abandoned their shopping cart, then consumers will be more likely to revisit and purchase.</p><p><strong>Step Four:</strong> Create the algorithm based on a cohesive message. Once you know the objective, customer segment, and the hypothesis that will be behind the process, it’s possible to create the type of cohesive messaging necessary for success. As the algorithm learns to respond based on the actions of the consumer, the potential responses need to feel coherent and authentic, so the user feels understood rather than manipulated.</p><p><strong>Step Five:</strong> Test within a controlled environment such as a website or email list — an atmosphere that allows for the control of variables. Using a third-party site to test native applications leaves a lot open to chance and may not result in satisfactory results or understanding of those results. Once you set your parameters, machine learning makes the process of testing new approaches infinitely easier than the previously required manual effort.</p><p>Using this five-step process, you can learn what works and discard whatever does not. Then, with a responsive and observant team, your company has the benefit of instant analyzation and human insight. Suddenly, machine learning is far more than just a buzzword; it’s valued data intelligence for both your business and your customers.</p>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : Five Steps to Approaching Machine Learning for Your Business
[22-Jun-2017 20:19:34 UTC] keyword: microsoft not found in : <figure><img alt="" src="https://cdn-images-1.medium.com/max/550/0*S8BzIAwjXc0XT-hR.png"/></figure><p>When we think about some particular person, very often we imagine his or her face, smile and different unique facial features. We know how to recognize the person by it’s face, understand his emotions, estimate his age, almost with 100% certainty tell his gender. Human vision system can do these and many other things with faces very easily. Can we do the same with modern artificial intelligence algorithms, in particular, deep neural networks ? In this article I would like to describe most of the common tasks in facial analysis, also providing references to already existing datasets and deep learning based solutions. The latter will allow you to play by your own, trying to apply these features for your business, or, better… you can contact us :)</p><h3><strong>Face recognition</strong></h3><p>This problem is the most basic one — how to find the face on the image? First, we would like just find the face in a rectangle, like on a picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EXhU34T_pqnK22va.jpg"/></figure><p>This problem is for years more or less successfully solved with Haar cascades that are implemented in popular OpenCV package, but here are alternatives:</p><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" rel="noreferrer" target="_blank"><strong>WIDER FACE: A Face Detection Benchmark</strong></a></p><p>One of the biggest datasets for faces, it consists of 32,203 images and label 393,703 faces in different conditions, so it’s good choice to try to train a detection model on it</p><p><a href="http://vis-www.cs.umass.edu/fddb/" rel="noreferrer" target="_blank"><strong>Face Detection Data Set and Benchmark</strong></a></p><p>Another good dataset, but smaller, with 5171 faces, good to test simple models on it.</p><p>What about algorithms and approaches for detection problem? I would offer you to check the <a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">following article</a> to see comparison of different approaches, but general and really good one is <a href="https://github.com/ShaoqingRen/faster_rcnn" rel="noreferrer" target="_blank"><strong>Faster RCNN</strong></a>, that is designed for object detection and classification pipeline and showed good results on VOC and ImageNet datasets for general image recognition and detection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*FxljGM0ZA4kqfsYMA1HMgQ.png"/><figcaption><a href="https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718" rel="noreferrer" target="_blank">https://medium.com/@phelixlau/speed-accuracy-trade-offs-for-modern-convolutional-object-detectors-bbad4e4e0718</a></figcaption></figure><h3>Face identification</h3><p>After we successfully cropped our face from the general image, most probably we would like to identify a person, for example, to match it with someone form our database.</p><p>Good point to start with face identification problem is to play with <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/" rel="noreferrer" target="_blank"><strong>VGG Face dataset</strong></a> — they have 2,622 identities in it. Moreover, they already provide <a href="http://www.robots.ox.ac.uk/~vgg/software/vgg_face/" rel="noreferrer" target="_blank">trained models and code</a> that you can use as feature extractors for your own machine learning pipeline. They also introduce and important concept as <em>triplet loss</em>, that you might use for large scale face identification. In two words, it “pushes” similar faces to have similar representation and does the opposite to different faces.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7xamdCNflBGRlvJq4vVoCA.png"/><figcaption>Triplet loss formula from VGG Face paper</figcaption></figure><p>If you want to go deeper both in terms of complexity of neural network and scale of data, you might try <a href="http://www.openu.ac.il/home/hassner/projects/augmented_faces/" rel="noreferrer" target="_blank"><strong>Do We Really Need to Collect Millions of Faces for Effective Face Recognition</strong></a><strong>. </strong>They use deeper ResNet-101 network with <a href="https://github.com/iacopomasi/face_specific_augm" rel="noreferrer" target="_blank">code and trained models</a> as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IdtrQ5h06gPYDT_h.png"/><figcaption>Augmenting faces by using different generic 3D models for rendering</figcaption></figure><p>I would like to remind, that you can use these models just as facial feature extractor, and apply given representations to your own purposes, for example, clustering, or metric-based methods.</p><h3>Facial keypoints detection</h3><p>Long story short, facial keypoints are <em>the most important points</em>, according with some metric, see, e.g., <a href="http://dl.acm.org/citation.cfm?id=954342" rel="noreferrer" target="_blank">Face recognition: A literature survey</a> , that can be extrapolate from a given (picture of a ) face to describe the associated emotional state, to improve a medical analysis, etc.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/286/0*Y8xTBH-eaIgdK5R4.png"/><figcaption>Facial keypoints example</figcaption></figure><p>If you really want to train a neural network model for keypoint detection, you may check <a href="https://www.kaggle.com/c/facial-keypoints-detection" rel="noreferrer" target="_blank"><strong>Kaggle dataset</strong></a> with <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" rel="noreferrer" target="_blank"><strong>corresponding tutorial</strong></a>, but maybe more convenient way to extract these points will be use of open-source library <a href="http://dlib.net/" rel="noreferrer" target="_blank"><strong>dlib</strong></a>.</p><h3>Age estimation and gender recognition</h3><p>Another interesting problem is understanding the gender of a person on the photo and try to estimate it’s age.</p><p><a href="http://www.openu.ac.il/home/hassner/Adience/data.html#agegender" rel="noreferrer" target="_blank"><strong>Unfiltered faces for gender and age classification</strong></a> — good dataset that provides 26,580 photos for 8 (0–2, 4–6, 8–13, 15–20, 25–32, 38–43, 48–53, 60-) age labels with corresponding labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/776/0*Br0h9qDKHbszZkDG.png"/><figcaption>Unfiltered faces for gender and age classification project page image</figcaption></figure><p>If you want to play with ready trained models you can check <a href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" rel="noreferrer" target="_blank"><strong>The Open University of Israel project</strong></a> or this <a href="https://github.com/oarriaga/face_classification" rel="noreferrer" target="_blank">Github page</a>.</p><h3>Emotions recognition</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*ooH7JLgG-CDvF1DV.jpg"/></figure><p>Emotion recognition from a photo is such a popular task, so it’s even implemented in some cameras, aiming at automatically detecting when you’re smiling. The same can be done by suitably training neural networks. There are two datasets: one from <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data" rel="noreferrer" target="_blank"><strong>Kaggle competition</strong></a> and<a href="http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main" rel="noreferrer" target="_blank"><strong>Radboud Faces Database</strong></a> that contain photos and labels for seven basic emotional states (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).</p><p>If you want to try some ready solution, you can try <a href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" rel="noreferrer" target="_blank"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></a>project, they provide code and models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/706/0*sE5LuyHDC3F8Yq0E.png"/><figcaption>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns project page image</figcaption></figure><p>The interesting moment of latter project is converting input image into local binary pattern space, after mapping it into 3D space and training a convolutional neural network on it.</p><h3>Facial action units detection</h3><p>Facial emotion recognition datasets usually have one problem — they are concentrated on learning an emotion only in one particular moment and the emotion has to be really visible, while in real life our smile or sad eye blink can be really short and subtle. And there is a special system for coding different facial expression parts into some “action units” — Facial action coding system (FACS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/748/1*Oh3FqSZ8YyUdac7Ep3jOOw.png"/><figcaption>Main facial action units table from <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System" rel="noreferrer" target="_blank">Wikipedia page</a></figcaption></figure><p>One of the most famous databases for action units (AUs) detection is <a href="http://www.pitt.edu/~emotion/ck-spread.htm" rel="noreferrer" target="_blank"><strong>Cohn-Kanade AU-Coded Expression Database</strong></a> with 486 sequences of emotional states. Another dataset is <a href="https://www.ecse.rpi.edu/~cvrl/database_zw.html" rel="noreferrer" target="_blank"><strong>RPI ISL Facial Expression Databases</strong></a>, which is licensed. Training a model to detect AUs keep in mind, that this is multilabel problem (several AUs are appearing is single example) and it can cause to different problems.</p><h3>Gaze capturing</h3><p>Another interesting and challenging problem is the so called <em>gaze tracking</em>. Detection of eye movements can be as a part of emotion detection pipeline, medical applications, but it’s an amazing way to make human-computer interfaces based on sights or a tool for retail and client engagement research.</p><p>Nice deep learning approach project is University of Georgia’ <a href="http://gazecapture.csail.mit.edu/index.php" rel="noreferrer" target="_blank"><strong>Eye Tracking for Everyone</strong></a><strong>, </strong>where they publish both dataset and trained model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Uhl_XjwfMfbZ9uJXU-m6mg.png"/><figcaption><a href="http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf" rel="noreferrer" target="_blank">http://gazecapture.csail.mit.edu/cvpr2016_gazecapture.pdf</a></figcaption></figure><h3>Conclusion</h3><p>Facial analysis is still very young area, but as you can see, there are a lot of developments and published models that you can try and test as a starting point of your own research. New databases are regularly appearing, so you can use transfer learning along with fine tuning already existing models, exploiting new datasets for your own tasks.</p><p>Stay tuned!</p><p><em>Alex Honchar</em><br/><strong>Cofounder and CTO</strong><br/>HPA | High Performance Analytics<br/>info: alex.gonchar@hpa8.com<br/><a href="http://www.hpa8.com/" rel="noreferrer" target="_blank">www.hpa8.com</a></p>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : Deep learning for facial analysis
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : <p>Today we’re going to have a look at an interesting set of learning algorithms which does not require you to know the truth while you learn. As such this is a mix of unsupervised and supervised learning. The supervised part comes from the fact that you look in the rear view mirror after the actions have been taken and then adapt yourself based on how well you did. This is surprisingly powerful as it can learn whatever the knowledge representation allows it to. One caveat though is that it is excruciatingly sloooooow. This naturally stems from the fact that there is no concept of a right solution. Neither when you are making decisions nor when you are evaluating them. All you can say is that “Hey, that wasn’t so bad given what I tried before” but you cannot say that it was the best thing to do. This puts a dampener on the learning rate. The gain is that we can learn just about anything given that we can observe the consequence of our actions in the environment we operate in.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/530/0*N5EgR1oTCS-HrLgF.png"/></figure><p>As illustrated above, reinforcement learning can be thought of as an agent acting in an environment and receiving rewards as a consequence of those actions. This is in principle a Markov Decision Process (MDP) which basically captures just about anything you might want to learn in an environment. Formally the MDP consists of</p><ul><li>A set of states</li><li>A set of actions</li><li>A set of rewards</li><li>A set of transition probabilities</li></ul><p>which looks surprisingly simple but is really all we need. The mission is to learn the best transition probabilities that maximizes the expected total future reward. Thus to move on we need to introduce a little mathematical notation. First off we need a reward function R(<strong>s</strong>, <strong>a</strong>) which gives us the reward <strong>r</strong> that comes from taking action <strong>a</strong> in state <strong>s</strong> at time <strong>t</strong>. We also need a transition function S(<strong>s</strong>, <strong>a</strong>) which will give us the next state <strong>s’</strong>. The actions <strong>a</strong> are generated by the agent by following one or several policies. A policy function P(<strong>s</strong>) therefore generates an action <strong>a</strong> which will, to it’s knowledge, give the maximum reward in the future.</p><h3>The problem we will solve — Cart Pole</h3><p>We will utilize an environment from the <a href="https://gym.openai.com/" rel="noreferrer" target="_blank">OpenAI Gym</a> called the <a href="https://gym.openai.com/envs/CartPole-v0" rel="noreferrer" target="_blank">Cart pole</a> problem. The task is basically learning how to balance a pole by controlling a cart. The environment gives us a new state every time we act in it. This state consists of four observables corresponding to position and movements. This problem has been illustrated before by <a href="https://gist.github.com/awjuliani/86ae316a231bceb96a3e2ab3ac8e646a" rel="noreferrer" target="_blank">Arthur Juliani</a> using <a href="https://www.tensorflow.org/" rel="noreferrer" target="_blank">TensorFlow</a>. Before showing you the implementation we’ll have a look at how a trained agent performs below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*lpdEyKze3XRSW_Yy.gif"/></figure><p>As you can see it performs quite well and actually manages to balance the pole by controlling the cart in real time. You might think that hey that sounds easy I’ll just generate random actions and it should cancel out. Well, put your mind at ease. Below you can see an illustration of that approach failing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*kBGJ26l28fThJhfb.gif"/></figure><p>So to the problem at hand. How can we model this? We need to make an agent that learns a policy that maximizes the future reward right? Right, so at any given time our policy can choose one of two possible actions namely</p><p>which should sound familiar to you if you’ve done any modeling before. This is basically a Bernoulli model where the probability distribution looks like this P(<strong>y</strong>;<strong>p</strong>)=<strong>p</strong>^<strong>y</strong>(1-<strong>p</strong>)^{1-<strong>y</strong>}. Once we know this the task is to model <strong>p</strong> as a function of the current state <strong>s</strong>. This can be done by doing a linear model wrapped by a sigmoid. For more mathematical details please have a look at my <a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank">original post</a>.</p><h3>Implementation</h3><p>As the AI Gym is mostly available in Python we’ve chosen to go with that language. This is by no means my preferred language for data science, and I could give you 10 solid arguments as to why it shouldn’t be yours either, but since this post is about machine learning and not data science I won’t expand my thoughts on that. In any case, Python is great for machine learning which is what we are looking at today. So let’s go ahead and import the libraries in Python3 that we’re going to need.</p><pre>import numpy as np<br/>import math<br/>import gym</pre><p>After this let’s look at initiating our environment and setting some variables and placeholders we are going to need.</p><pre>env = gym.make(&#039;CartPole-v0&#039;)<br/># Configuration<br/>state = env.reset()<br/>max_episodes = 2000<br/>batch_size = 5<br/>learning_rate = 1<br/>episodes = 0<br/>reward_sum = 0<br/>params = np.random.normal([0,0,0,0], [1,1,1,1])<br/>render = False<br/># Define place holders for the problem<br/>p, action, reward, dreward = 0, 0, 0, 0 ys, ps, actions, rewards, drewards, gradients = [],[],[],[],[],[]<br/>states = state</pre><p>Other than this we’re going to use some functions that needs to be defined. I’m sure multiple machine learning frameworks have implemented it already but it’s pretty easy to do and quite instructional so why not just do it. ;)</p><h3>The python functions you’re going to need</h3><p>As we’re implementing this in Python3 and it’s not always straightforward what is Python3 and Python2 I’m sharing the function definitions with you that I created since they are indeed compliant with the Python3 libraries. Especially Numpy which is an integral part of computation in Python. Most of these functions are easily implemented and understood. Make sure you read through them and grasp what they’re all about.</p><pre>def discount_rewards(r, gamma=1-0.99):<br/>df = np.zeros_like(r)<br/>for t in range(len(r)):<br/>df[t] = np.npv(gamma, r[t:len(r)])<br/>return df<br/>def sigmoid(x):<br/>return 1.0/(1.0+np.exp(-x))<br/>def dsigmoid(x): <br/>a=sigmoid(x) return a*(1-a)<br/>def decide(b, x):<br/>return sigmoid(np.vdot(b, x))<br/>def loglikelihood(y, p):<br/>return y*np.log(p)+(1-y)*np.log(1-p)<br/>def weighted_loglikelihood(y, p, dr):<br/>return (y*np.log(p)+(1-y)*np.log(1-p))*dr<br/>def loss(y, p, dr):<br/>return -weighted_loglikelihood(y, p, dr)<br/>def dloss(y, p, dr, x):<br/>return np.reshape(dr*( (1-np.array(y))*p - y*(1-np.array(p))),[len(y),1])*x</pre><p>Armed with these function we’re ready to do the main learning loop which is where the logic of the agent and the training takes place. This will be the heaviest part to run through so take your time.</p><h3>The learning loop</h3><pre>while episodes &lt; max_episodes:<br/>if reward_sum &gt; 190 or render==True: <br/>env.render()<br/>render = True<br/>p = decide(params, state)<br/>action = 1 if p &gt; np.random.uniform() else 0<br/>state, reward, done, _ = env.step(action) reward_sum += reward<br/># Add to place holders<br/>ps.append(p)<br/>actions.append(action)<br/>ys.append(action)<br/>rewards.append(reward)<br/># Check if the episode is over and calculate gradients<br/>if done:<br/>episodes += 1<br/>drewards = discount_rewards2(rewards)<br/>drewards -= np.mean(drewards)<br/>drewards /= np.std(drewards)<br/>if len(gradients)==0:<br/>gradients = dloss(ys, ps, drewards, states).mean(axis=0)<br/>else:<br/>gradients = np.vstack((gradients, dloss(ys, ps, drewards, states).mean(axis=0)))<br/>if episodes % batch_size == 0:<br/>params = params - learning_rate*gradients.mean(axis=0)<br/>gradients = []<br/>print(&quot;Average reward for episode&quot;, reward_sum/batch_size)<br/>if reward_sum/batch_size &gt;= 200:<br/>print(&quot;Problem solved!&quot;)<br/>reward_sum = 0<br/># Reset all<br/>state = env.reset()<br/>y, p, action, reward, dreward, g = 0, 0, 0, 0, 0, 0<br/>ys, ps, actions, rewards, drewards = [],[],[],[],[]<br/>states = state<br/>else: <br/>states=np.vstack((states, state))</pre><pre>env.close()</pre><p>Phew! There it was, and it wasn’t so bad was it? We now have a fully working reinforcement learning agent that learns the CartPole problem by policy gradient learning. Now, for those of you who know me you know I’m always preaching about considering all possible solutions that are consistent with your data. So maybe there are more than one solution to the CartPole problem? Indeed there is. The next section will show you a distribution of these solutions across the four parameters.</p><h3>Multiple solutions</h3><p>So we have solved the CartPole problem using our learning agent and if you run it multiple times you will see that it converges to different solutions. We can create a distribution over all of these different solutions which will inform us about the solution space of all possible models supported by our parameterization. The plot is given below where the x axis are the parameter values and the y axis the probability density.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iPlTnHQ_9DfzO_KY.png"/></figure><p>You can see that X0 and X1 should be around 0 meanwhile X2 and X3 should be around 1. But several other solutions exist as illustrated. So naturally this uncertainty about what the parameters should exactly be could be taken into account by a learning agent.</p><h3>Conclusion</h3><p>We have implemented a reinforcement learning agent who acts in an environment with the purpose of maximizing the future reward. We have also discounted that future reward in the code but not covered it in the math. It’s straightforward though. The concept of being able to learn from your own mistakes is quite cool and represents a learning paradigm which is neither supervised nor unsupervised but rather a combination of both. Another appealing thing about this methodology is that it is very similar to how biological creatures learn from interacting with their environment. Today we solved the CartPole but the methodology can be used to attack far more interesting problems.</p><p>I hope you had fun reading this and learned something.</p><p>Happy inferencing!</p><p><em>Originally published at </em><a href="http://doktormike.github.io/blog/A-gentle-introduction-to-reinforcement-learning-or-what-to-do-when-you-dont-know-what-to-do/" rel="noreferrer" target="_blank"><em>doktormike.github.io</em></a><em>.</em></p>
[22-Jun-2017 20:19:34 UTC] keyword: turing not found in : A gentle introduction to reinforcement learning or what to do when you don’t know what to do
